###############################################################################
##                                                                           ##
##  Title       : Replication material for BOTS survey descriptives          ##
##                                                                           ##
##  Author      : Ieva Rozentale                                             ##
##  Affiliation : Amsterdam Business School                                  ##
##  Version     : 10/09/2018                                                 ##
##  N           : 179                                                        ## 
##  Description : Code for generating sample descriptives for articles       ## 
##                and other interesting insights for reports                 ## 
##                                                                           ## 
##                                                                           ## 
###############################################################################

# Load the packages into R
library(dplyr)
library(knitr)
library(corrr)
library(readr)
library(tidyr)
library(devtools)
library(plyr)
library(tidyselect)
library(QCA)
library(SetMethods)
library(graphics)

bots<- read_csv("~/Dropbox/1PhD/Articles/Survey Data/bots_clean_small.csv") #MAC

createdata <- function(x) {
  bots %>%
    dplyr::select(grep(x, names(bots)))
}

#make a grouping variable - size
bots$size <- class(factor)
bots$size[bots$Q1_4 >= 250] = "Large"
bots$size[bots$Q1_4 < 250] = "Medium"
bots$size[bots$Q1_4 < 50] = "Small"
bots$size[bots$Q1_4 < 10] = "Micro"
bots$size <- factor(bots$size, levels=c("Micro", "Small", "Medium", "Large"))

#make a sector variable (9 - levels)
sectors2<- c(4,2,2,9,5,5,4,5,1,2,4,5,1,5,3,4,4,2,6,5,7,1,6,4,1,6,4,6,6,4,8,5,5,1,3,9,1,1,4,4,7,1,1,9,1,1,5,4,4,4,4,9,9,4,4,3,1,5,9,4,2,3,4,4,5,5,1,4,4,5,4,4,4,4,4,6,1,5,5,4,6,6,4,4,4,6,4,2,1,3,4,1,1,7,4,2,4,7,4,4,6,6,1,5,1,4,2,3,9,9,6,6,4,4,1,9,6,9,1,4,1,2,5,2,5,4,5,6,4,5,5,4,4,4,2,9,2,4,5,3,4,9,7,3,5,4,5,9,4,5,5,6,2,5,4,6,4,4,4,4,5,5,5,4,6,3,7,5,2,4,6,7,4,1,6,5,4,4,5)
bots$sector <- sectors2
bots$sector[bots$sector == 1] = "Architecture & urban planning"
bots$sector[bots$sector == 2] = "Products"
bots$sector[bots$sector == 3] = "Audiovisual services & animation"
bots$sector[bots$sector == 4] = "Graphic, communication & digital design"
bots$sector[bots$sector == 5] = "Advertisement & marketing"
bots$sector[bots$sector == 6] = "Spatial design & interior architecture"
bots$sector[bots$sector == 7] = "Industrial design"
bots$sector[bots$sector == 8] = "Games & design"
bots$sector[bots$sector == 9] = "Mixed creative services & strategy"
bots$sector <- as.factor(bots$sector)

#Adding performance variables
bots$CP <- rowMeans(bots[,c("Q20_CP_1", "Q20_CP_2", "Q20_CP_9", "Q20_CP_10")]) ##Adding creative performance to data frame
bots$BP <- rowMeans(bots[,  c("Q20_BP_1", "Q20_BP_3", "Q20_BP_10")]) ##Adding business performance to data frame

#Grouping variable for performance based on Likert categories
bots$performance <- class(factor)
bots$performance[bots$CP > 4 & bots$BP >4 ] = "Both high"
bots$performance[bots$CP <= 4 & bots$BP <= 4 ] = "Both low"
bots$performance[bots$CP > 4 & bots$BP <= 4 ] = "High creative"
bots$performance[bots$CP <= 4 & bots$BP > 4 ] = "High business"
bots$performance <- factor(bots$performance, 
                           levels=c("Both high", "Both low", "High creative", "High business"))
tibble(bots$sector, bots$performance)

#--> experiment with counts using table and prop.table functions to see how proportianlly there are of each
#then compare to the TFR calibration

###Correlate performances with objective measures
objectiveperformance <- as.data.frame(createdata("Q17"))
perf <- cbind(objectiveperformance[,c(1:5, 11:13)], bots[,c("CP","BP")])

cor(perf, use = "complete.obs") %>% round(2)

x <- corstars(perf)
#It looks like the only significant performance is between CP and BP, 
#and between the BP and Q17_8 - the "percentage of yearly income from international projects"  



#Grouping variable for performance based on percentiles
Percentile_00  = min(bots$CP)
Percentile_50  = quantile(bots$CP, 0.5)
Percentile_100 = max(bots$CP)

RB = rbind(Percentile_00, Percentile_50, Percentile_100)

dimnames(RB)[[2]] = "Value"

RB

bots$CPGroup <- class(factor)
bots$CPGroup[bots$CP >= Percentile_00 & bots$CP <  Percentile_50]  = "Lower_half"
bots$CPGroup[bots$CP >= Percentile_50 & bots$CP <= Percentile_100] = "Upper_half"
bots$CPGroup = factor(bots$CPGroup,
                    levels=c("Lower_half", "Upper_half"))

Percentile_00  = min(bots$BP)
Percentile_50  = quantile(bots$BP, 0.5)
Percentile_100 = max(bots$BP)

RB = rbind(Percentile_00, Percentile_50, Percentile_100)

dimnames(RB)[[2]] = "Value"

RB

bots$BPGroup <- class(factor)
bots$BPGroup[bots$BP >= Percentile_00 & bots$BP <  Percentile_50]  = "Lower_half"
bots$BPGroup[bots$BP >= Percentile_50 & bots$BP <= Percentile_100] = "Upper_half"
bots$BPGroup = factor(bots$BPGroup,
                      levels=c("Lower_half", "Upper_half"))

tibble(bots$CPGroup, bots$BPGroup, bots$performance)

rm("RB", "Percentile_00", "Percentile_50", "Percentile_50", "sectors2")

##Judging from the first ten rows of the tibble both categorization strategies give the same results 
##(good for my calibration scores - a conclusion that can be used in robustness tests).

#Compare both 


#Validity of integration/differentiation scale
paradox <- bots[,grepl("Q16", names(bots))]
mydata2 <- paradox[,1:12]
colnames(mydata2) <- paste("X", 1:12, sep="")

## MODEL specification
HS.modelPAR <- 'integration  =~ X1 + X3 + X5 +  X8 + X9 + X12
differentiation =~ X2 + X4 + X6 + X7 + X10 + X11'

fitPAR <- cfa(HS.modelPAR, data=mydata2)
summary(fitPAR, fit.measures=TRUE)

model.reliabilityPAR <- round(reliability(fitPAR), digits = 3)
print(model.reliabilityPAR)

parameterEstimates(fitPAR)
inspect(fitPAR,what="std")$lambda

#Removing the items that don't perform well enough
HS.modelPER2 <- 'int1  =~ X3 + X12 
dif =~X4 + X7 + X11'

fitPER2 <- cfa(HS.modelPER2, data=mydata2)
summary(fitPER2, fit.measures=TRUE)

model.reliabilityPER2 <- round(reliability(fitPER2), digits = 3)
print(model.reliabilityPER2)

parameterEstimates(fitPER2)
inspect(fitPER2,what="std")$lambda

#these match with general approaches (would it be valid to say that the rest can be seen as individual approaches
#to paradox management?) 
#int - 2,6
#bp - 2,4,6


#Finding clusters of 

Data.num = Data[c("Happy", "Tired")]


#Code for plotting the distribution of the variables 
densityplot(~Q2_BM1_1+Q2_BM1_2+Q2_BM1_3, data=bots, 
            plot.points=FALSE, ref=TRUE, 
            xlab="Distribution of offering related variables",
            auto.key = list(space = "right"))

#================================================
#VIZUALIZATIONS THAT WOULD BE INTERESTING TO SEE
#================================================

#Code for vizualizing data per groups (e.g. high performing / low performing )
bots[c("Q2_BM1_1", "Q2_BM1_2", "Q2_BM1_3")] <- lapply(bots[c("Q2_BM1_1", "Q2_BM1_2", "Q2_BM1_3")], factor, levels=1:7)
tryout <-as.data.frame(bots[c("Q2_BM1_1", "Q2_BM1_2", "Q2_BM1_3", "performance")])
tryout[1:3] <- lapply(tryout[1:3], factor, levels=1:7)
likt <- likert(tryout[1:3], grouping = tryout$performance)
plot(likt)

bots[c("Q3_BM2_1", "Q3_BM2_2", "Q3_BM2_3")] <- lapply(bots[c("Q3_BM2_1", "Q3_BM2_2", "Q3_BM2_3")], factor, levels=1:7)
tryout <-as.data.frame(bots[c("Q3_BM2_1", "Q3_BM2_2", "Q3_BM2_3","Q3_BM2_4","performance")])
tryout[1:4] <- lapply(tryout[1:4], factor, levels=1:7)
likt <- likert(tryout[1:4], grouping = tryout$performance)
plot(likt)


#Pie chart per dominant business model (according to the memberships to )

#
tab <- corstars(fsq[,2:9])



# x is a matrix containing the data
# method : correlation method. "pearson"" or "spearman"" is supported
# removeTriangle : remove upper or lower triangle
# results :  if "html" or "latex"
# the results will be displayed in html or latex format
corstars <-function(x, method=c("pearson", "spearman"), removeTriangle=c("upper", "lower"),
                    result=c("none", "html", "latex")){
  #Compute correlation matrix
  require(Hmisc)
  x <- as.matrix(x)
  correlation_matrix<-rcorr(x, type=method[1])
  R <- correlation_matrix$r # Matrix of correlation coeficients
  p <- correlation_matrix$P # Matrix of p-value 
  
  ## Define notions for significance levels; spacing is important.
  mystars <- ifelse(p < .0001, "****", ifelse(p < .001, "*** ", ifelse(p < .01, "**  ", ifelse(p < .05, "*   ", "    "))))
  
  ## trunctuate the correlation matrix to two decimal
  R <- format(round(cbind(rep(-1.11, ncol(x)), R), 2))[,-1]
  
  ## build a new matrix that includes the correlations with their apropriate stars
  Rnew <- matrix(paste(R, mystars, sep=""), ncol=ncol(x))
  diag(Rnew) <- paste(diag(R), " ", sep="")
  rownames(Rnew) <- colnames(x)
  colnames(Rnew) <- paste(colnames(x), "", sep="")
  
  ## remove upper triangle of correlation matrix
  if(removeTriangle[1]=="upper"){
    Rnew <- as.matrix(Rnew)
    Rnew[upper.tri(Rnew, diag = TRUE)] <- ""
    Rnew <- as.data.frame(Rnew)
  }
  
  ## remove lower triangle of correlation matrix
  else if(removeTriangle[1]=="lower"){
    Rnew <- as.matrix(Rnew)
    Rnew[lower.tri(Rnew, diag = TRUE)] <- ""
    Rnew <- as.data.frame(Rnew)
  }
  
  ## remove last column and return the correlation matrix
  Rnew <- cbind(Rnew[1:length(Rnew)-1])
  if (result[1]=="none") return(Rnew)
  else{
    if(result[1]=="html") print(xtable(Rnew), type="html")
    else print(xtable(Rnew), type="latex") 
  }
} 
