summarise(Niche = mean(Niche),
General = mean(General),
B2B = mean(B2B),
B2C = mean(B2C),
B2G = mean(B2G),
CoCreation = mean(CoCreation)
)
clients
a <- c("NC", "VAR", "Value")
valueslow <- as.numeric(clients[1, 2:7])
valueshigh <- as.numeric(clients[2, 2:7])
low <- rep("low", 6)
high <- rep("high", 6)
VAR <- colnames(clients[-1])
Value <- c(valueslow, valueshigh)
Value <- round(Value, digits = 2)
VAR <- rep(VAR, 2)
NC <- c(low, high)
barplotdf <- cbind(NC, Value, VAR)
barplotdf <- as.data.frame(barplotdf)
plot <- ggplot(barplotdf, aes(x=barplotdf$VAR, y=barplotdf$Value, group = barplotdf$NC, fill=barplotdf$NC)) +
geom_bar(stat="identity", position="dodge")
plot <- plot +
(labs(title = "Client Attraction x Client Segments",
caption = "Source: BOTS Survey UvA 2017/18",
x = "Indicators",
y = "Average Score"))
plot <- plot + labs(fill = "Client Attraction")
plot <- plot + scale_fill_manual(values=mypal2)
plot
library(ggplot2)
library(ggalt)
install.packages("ggalt", lib="/Library/Frameworks/R.framework/Versions/3.4/Resources/library")
install.packages("ggfortify", lib="/Library/Frameworks/R.framework/Versions/3.4/Resources/library")
library(ggalt)
library(ggfortify)
theme_set(theme_classic())
View(bots_full)
View(bots_full)
#Function for creating data frames that contain specific parts
createdata <- function(x) {
bots_full %>%
dplyr::select(grep(x, names(bots_full)))
}
VC <- createdata("VC")
VC <- createdata("V")
VC <- createdata("Q15")
# Compute data with principal components ------------------
df <- VC
pca_mod <- prcomp(df)  # compute principal components
# Data frame of principal components ----------------------
df_pc <- data.frame(pca_mod$x, Species=iris$Species)  # dataframe of principal components
View(pca_mod)
View(pca_mod)
install.packages("twitteR", "ROAuth", "stringr")
library(twitteR)
install.packages("twitteR", lib="/Library/Frameworks/R.framework/Versions/3.4/Resources/library")
library(twitteR)
library(ROAuth)
install.packages("ROAuth", lib="/Library/Frameworks/R.framework/Versions/3.4/Resources/library")
library(plyr)
library(dplyr)
library(stringr)
library(ggplot2)
reqURL <- 'https://api.twitter.com/oauth/request_token'
accessURL <- 'https://api.twitter.com/oauth/access_token'
authURL <- 'https://api.twitter.com/oauth/authorize'
consumerKey <- 'AO3CuDOcxELsP6tSdqvwCKeYJ' #put the Consumer Key from Twitter Application
consumerSecret <- 'yM2fFRfuUoi7kACddhmyO5tq3UpOkboKyi6em7V0NhHLsKK7HK'  #put the Consumer Secret from Twitter Application
Cred <- OAuthFactory$new(consumerKey=consumerKey,
consumerSecret=consumerSecret,
requestURL=reqURL,
accessURL=accessURL,
authURL=authURL)
download.file(url='http://curl.haxx.se/ca/cacert.pem', destfile='cacert.pem')
Cred <- OAuthFactory$new(consumerKey=consumerKey,
consumerSecret=consumerSecret,
requestURL=reqURL,
accessURL=accessURL,
authURL=authURL)
knitr::include_graphics("Figure1.1.png")
knitr::include_graphics("Figure1.1.png")
knitr::include_graphics("Figure1.2.jpg")
knitr::include_graphics("FigureC2.png", dpi = NA)
#========================
#GETTING THE SURVEY DATA
#========================
bots<- read_csv("~/Dropbox/1PhD/Articles/Survey Data/bots_renamed.csv")
print(as.data.frame(test.simple$valid))
setwd("~/Dropbox/1PhD/Articles/Paper3/Survey Data")
install.packages("install.load")
library(install.load)
pkg <- c("corrr", "readr", "tidyr", "devtools", "dplyr", "plyr", "tidyselect", "e1071", "purrr", "FactoMineR", "psy", "psych", "GPArotation", "tibble", "boot", "ltm", "xtable", "kableExtra", "sem", "lavaan", "irr", "QCA", "SetMethods", "corrplot", "ggpubr", "mclust",
"semTools")
install_load(pkg)
bots<- read_csv("~/Dropbox/1PhD/Articles/Survey Data/bots_renamed.csv")
createdata <- function(x) {
bots %>%
dplyr::select(grep(x, names(bots)))
}
corstars <-function(x, method=c("pearson", "spearman"), removeTriangle=c("upper", "lower"),
result=c("none", "html", "latex")){
#Compute correlation matrix
require(Hmisc)
x <- as.matrix(x)
correlation_matrix<-rcorr(x, type=method[1])
R <- correlation_matrix$r # Matrix of correlation coeficients
p <- correlation_matrix$P # Matrix of p-value
## Define notions for significance levels; spacing is important.
mystars <- ifelse(p < .0001, "****", ifelse(p < .001, "*** ", ifelse(p < .01, "**  ", ifelse(p < .05, "*   ", "    "))))
## trunctuate the correlation matrix to two decimal
R <- format(round(cbind(rep(-1.11, ncol(x)), R), 2))[,-1]
## build a new matrix that includes the correlations with their apropriate stars
Rnew <- matrix(paste(R, mystars, sep=""), ncol=ncol(x))
diag(Rnew) <- paste(diag(R), " ", sep="")
rownames(Rnew) <- colnames(x)
colnames(Rnew) <- paste(colnames(x), "", sep="")
## remove upper triangle of correlation matrix
if(removeTriangle[1]=="upper"){
Rnew <- as.matrix(Rnew)
Rnew[upper.tri(Rnew, diag = TRUE)] <- ""
Rnew <- as.data.frame(Rnew)
}
## remove lower triangle of correlation matrix
else if(removeTriangle[1]=="lower"){
Rnew <- as.matrix(Rnew)
Rnew[lower.tri(Rnew, diag = TRUE)] <- ""
Rnew <- as.data.frame(Rnew)
}
## remove last column and return the correlation matrix
Rnew <- cbind(Rnew[1:length(Rnew)-1])
if (result[1]=="none") return(Rnew)
else{
if(result[1]=="html") print(xtable(Rnew), type="html")
else print(xtable(Rnew), type="latex")
}
}
#Make a data frame that only includes data for values variables
values <- bots[,grepl("values", names(bots))]
#After the examination, we took out the market value data, since they were highly skewed.
values <- values[,!grepl("Market", names(values))]
#Correlation matrix (lower part)
knitr::kable(corstars(values), booktabs = TRUE,
caption = "Correlation table, all items."
)
#Finding the number of factors
par <- fa.parallel(values) #analysis suggests 3 factors
#CFA using the psych package
test.simple <- fa(r=cor(values, use="complete.obs"), nfactors=3, fm="ml", rotate="oblimin")
fa.diagram(test.simple, cut = .4, digits = 2)
print(as.data.frame(test.simple$loadings))
print(as.data.frame(test.simple$valid))
print(as.data.frame(test.simple$valid, row.names = c("ML1", "ML2", "ML3")))
fa.diagram(ICLUST(values,2,title="Two cluster solution"),main="Input from ICLUST")
require(kableExtra)
require(knitr)
require(dplyr)
surveyvariables<- read.csv("~/Dropbox/1PhD/Articles/Survey Data/surveyquestions.csv")
surveyvariables[,-1] %>%
kable(row.names = F)
require(kableExtra)
require(knitr)
require(dplyr)
surveyvariables<- read.csv("~/Dropbox/1PhD/Articles/Survey Data/surveyquestions.csv")
surveyvariables[,-1] %>%
kable(row.names = F)
setwd("~/Dropbox/1PhD/Articles/Paper3/P3_EGOS18")
setwd("~/Dropbox/1PhD/Articles/Dissertation")
install.packages("install.load")
library(install.load)
pkg <- c("corrr", "readr", "tidyr", "devtools", "dplyr", "plyr", "tidyselect", "e1071", "purrr", "FactoMineR", "psy", "psych", "GPArotation", "tibble", "boot", "ltm", "xtable", "kableExtra", "sem", "lavaan", "irr", "QCA", "SetMethods", "corrplot", "ggpubr", "mclust")
install_load(pkg)
if (length(new.pkg)) {
install_load(new.pkg, repos = "http://cran.rstudio.com")
}
new.pkg <- pkg[!(pkg %in% installed.packages())]
if (length(new.pkg)) {
install_load(new.pkg, repos = "http://cran.rstudio.com")
}
bots<- read_csv("~/Dropbox/1PhD/Articles/Survey Data/bots_renamed.csv")
bots<- read_csv("~/Dropbox/1PhD/Articles/Dissertation/Data/bots_renamed.csv")
createdata <- function(x) {
bots %>%
dplyr::select(grep(x, names(bots)))
}
#Make a data frame that only includes data for values variables
values <- bots[,grepl("values", names(bots))]
#Check the distributions for non-normalities
apply(values, 2, hist, xlab= "Likert score")
#After the examination, we took out the market value data, since they were highly skewed.
values <- values[,!grepl("Market", names(values))]
#Correlation matrix (lower part)
lowerCor(values)
#Finding the number of factors
fa.parallel(values) #analysis suggests 3 factors
vss(values) #3 or 4 factors
#CFA using the psych package
test.simple <- fa(r=cor(values, use="complete.obs"), nfactors=3, fm="ml", rotate="oblimin")
test.simple
fa.diagram(test.simple, cut = .4, digits = 2)
test.simple$loadings
test.simple$valid
#using the clustering algorithm (a simpler alternative to CFA)
iclust(values)
Phi <- test.simple$Phi
fa.diagram(test.simple$loadings,Phi=Phi,main="Input from a matrix")
fa.diagram(ICLUST(values,3,title="Two cluster solution"),main="Input from ICLUST")
omega(values)
psych::alpha(values) #score all of the items as part of one scale.
keys.list <- list(creative=c(6,7,4,8,3,5,2), entrepreneurial=c(15,14,13,1),
financial=c(11,10,16,12,9))
myKeys <- make.keys(values,keys.list)
my.scores <- scoreItems(myKeys,values) #form several scales
print(my.scores, short = FALSE) #show the highlights of the results
psych::alpha(values) #score all of the items as part of one scale.
keys.list <- list(creative=c(6,7,4,8,3,5,2), entrepreneurial=c(15,14,13,1),
financial=c(11,10,16,12,9))
myKeys <- make.keys(values,keys.list)
my.scores <- scoreItems(myKeys,values) #form several scales
print(my.scores, short = FALSE) #show the highlights of the results
businessmodels <- bots[,grepl("BM", names(bots))] #selecting only business model questions
businessmodels <- businessmodels[,!grepl("Q7", names(businessmodels))] #partially answered question
businessmodels <- businessmodels[,!grepl("TEXT", names(businessmodels))] #remove open text variables
businessmodels <- businessmodels[,!grepl("Q11_BM10_8", names(businessmodels))] #only present in first survey
businessmodels <- businessmodels[,!grepl("Q12_BM11_6", names(businessmodels))] #only present in first survey
businessmodels$Q12_BM11_2[businessmodels$Q12_BM11_2 == '-99'] <- 1 #recode the bug in answers to low
businessmodels$Q12_BM11_2[businessmodels$Q12_BM11_3 == '-99'] <- 1
businessmodels$Q12_BM11_2[businessmodels$Q12_BM11_4 == '-99'] <- 1
businessmodels$Q12_BM11_2[businessmodels$Q12_BM11_5 == '-99'] <- 1
#First we conducetd a "parallel" analysis in order to to determine the number of components in a data matrix.
pcadat <- cor(businessmodels, use = "complete.obs") #correlation matrix
View(businessmodels)
businessmodels <- bots[,grepl("bm", names(bots))] #selecting only business model questions
businessmodels[businessmodels == '-99'] <- 1 #recode the bug in answers to low
businessmodels2 <- businessmodels[,colSums(is.na(businessmodels)) == 0] #remove columns that have NA's
businessmodels <- bots[,grepl("bm", names(bots))] #selecting only business model questions
businessmodels[businessmodels == '-99'] <- 1 #recode the bug in answers to low
businessmodels2 <- businessmodels[,colSums(is.na(businessmodels)) == 0] #remove columns that have NA's
businessmodels <- businessmodels[,!grepl("Other", names(businessmodels))]
businessmodels <- businessmodels[,1:41]
View(businessmodels)
#First we conducetd a "parallel" analysis in order to to determine the number of components in a data matrix.
pcadat <- cor(businessmodels, use = "complete.obs") #correlation matrix
#First we conducetd a "parallel" analysis in order to to determine the number of components in a data matrix.
pcadat <- cor(businessmodels, use = "complete.obs") #correlation matrix
businessmodels <- businessmodels[,!grepl("HourlyRates.1", names(businessmodels))]
#First we conducetd a "parallel" analysis in order to to determine the number of components in a data matrix.
pcadat <- cor(businessmodels, use = "complete.obs") #correlation matrix
parallel2 <- fa.parallel(pcadat, fm = 'minres', fa = 'both', n.obs = 179)
#based on the results we retain 6 principal? components
pca3 <- PCA(businessmodels, ncp=6, graph = FALSE)
parallel2 <- fa.parallel(pcadat, fm = 'minres', fa = 'both', n.obs = 190)
#based on the results we retain 6 principal? components
pca3 <- PCA(businessmodels, ncp=6, graph = FALSE)
pca3$eig
pcascores <- round(as.data.frame(pca3$ind$coord), 2) #using coordinates as membership scores to pca
View(pcascores)
#First we conducetd a "parallel" analysis in order to to determine the number of components in a data matrix.
pcadat <- cor(businessmodels, use = "complete.obs") #correlation matrix
parallel <- fa.parallel(pcadat, fm = 'minres', fa = 'both', n.obs = 190)
#based on the results we retain 6 principal? components
pca <- PCA(pcadat, ncp=6, graph = FALSE)  ###tried inserting the correlation matrix instead
summary(pca)
pca$eig
#based on the results we retain 6 principal? components
pca <- PCA(pcadat, ncp=5, graph = TRUE)  ###tried inserting the correlation matrix instead
#based on the results we retain 6 principal? components
pca <- PCA(pcadat, ncp=5, graph = FALSE)  ###tried inserting the correlation matrix instead
summary(pca)
pca$eig
#First we conducetd a "parallel" analysis in order to to determine the number of components in a data matrix.
pcadat <- cor(businessmodels, use = "complete.obs") #correlation matrix
parallel2 <- fa.parallel(pcadat, fm = 'minres', fa = 'both', n.obs = 190)
#based on the results we retain 6 principal? components
pca3 <- PCA(businessmodels, ncp=6, graph = FALSE)
pca3$eig
#based on the results we retain 6 principal? components
pca <- PCA(pcadat, ncp=5, graph = FALSE)  ###tried inserting the correlation matrix instead
summary(pca)
pca$eig
#The loadings matrices
loadingspca <- pca$var$cor # 6 is all components, because that was suggested by the analysis
#The scores matrices
scorespca <- pca$ind$coord#for 6 components
###Component loadings on
varcontributions <- as.data.frame(loadingspca)
comp1 <- subset(varcontributions, Dim.1 >= 0.7)
comp1 <- as.vector(rownames(comp1))
comp2 <- subset(varcontributions, Dim.2 >= 0.5)
comp2 <- as.vector(rownames(comp2))
comp3 <- subset(varcontributions, Dim.3 >= 0.5)
comp3 <- as.vector(rownames(comp3))
comp4 <- subset(varcontributions, Dim.4 >= 0.45)
comp4 <- as.vector(rownames(comp4))
comp5 <- subset(varcontributions, Dim.5 >= 0.40)
comp5 <- as.vector(rownames(comp5))
comp6 <- subset(varcontributions, Dim.6 >= 0.40)
comp6 <- as.vector(rownames(comp6))
#MAKE A TABLE OF THE RELEVANT COMPONENTS
x <- as.data.frame(cbind(comp1, comp2, comp3, comp4, comp5, comp6))
x$comp2[8] <- "-"
x$comp3[5:8] <- "-"
x$comp4[3:8] <- "-"
x$comp5[2:8] <- "-"
x$comp6[3:8] <- "-"
####TABLE SHOWING WHICH VARIABLES ACCOUNT THE MOST FOR THE FEATURES OF EACH COMPONENT
print(x)
###Component loadings on
varcontributions <- as.data.frame(loadingspca)
comp1 <- subset(varcontributions, Dim.1 >= 0.7)
comp1 <- as.vector(rownames(comp1))
comp2 <- subset(varcontributions, Dim.2 >= 0.5)
comp2 <- as.vector(rownames(comp2))
comp3 <- subset(varcontributions, Dim.3 >= 0.5)
comp3 <- as.vector(rownames(comp3))
comp4 <- subset(varcontributions, Dim.4 >= 0.45)
comp4 <- as.vector(rownames(comp4))
comp5 <- subset(varcontributions, Dim.5 >= 0.40)
comp5 <- as.vector(rownames(comp5))
comp6 <- subset(varcontributions, Dim.6 >= 0.40)
#based on the results we retain 6 principal? components
pca <- PCA(pcadat, ncp=6, graph = FALSE)  ###tried inserting the correlation matrix instead
summary(pca)
pca$eig
#The loadings matrices
loadingspca <- pca$var$cor # 6 is all components, because that was suggested by the analysis
#The scores matrices
scorespca <- pca$ind$coord#for 6 components
###Component loadings on
varcontributions <- as.data.frame(loadingspca)
comp1 <- subset(varcontributions, Dim.1 >= 0.7)
comp1 <- as.vector(rownames(comp1))
comp2 <- subset(varcontributions, Dim.2 >= 0.5)
comp2 <- as.vector(rownames(comp2))
comp3 <- subset(varcontributions, Dim.3 >= 0.5)
comp3 <- as.vector(rownames(comp3))
comp4 <- subset(varcontributions, Dim.4 >= 0.45)
comp4 <- as.vector(rownames(comp4))
comp5 <- subset(varcontributions, Dim.5 >= 0.40)
comp5 <- as.vector(rownames(comp5))
comp6 <- subset(varcontributions, Dim.6 >= 0.40)
comp6 <- as.vector(rownames(comp6))
#MAKE A TABLE OF THE RELEVANT COMPONENTS
x <- as.data.frame(cbind(comp1, comp2, comp3, comp4, comp5, comp6))
x$comp2[8] <- "-"
x$comp3[5:8] <- "-"
x$comp4[3:8] <- "-"
x$comp5[2:8] <- "-"
x$comp6[3:8] <- "-"
####TABLE SHOWING WHICH VARIABLES ACCOUNT THE MOST FOR THE FEATURES OF EACH COMPONENT
print(x)
View(x)
#First we conducetd a "parallel" analysis in order to to determine the number of components in a data matrix.
pcadat <- cor(businessmodels, use = "complete.obs") #correlation matrix
parallel <- fa.parallel(pcadat, fm = 'minres', fa = 'both', n.obs = 190)
pca <- PCA(pcadat, ncp=6, graph = FALSE)  ###tried inserting the correlation matrix instead
summary(pca)
pca$eig
setwd("~/Dropbox/1PhD/Articles/Dissertation")
bots<- read_csv("~/Dropbox/1PhD/Articles/Dissertation/Data/bots_renamed.csv")
createdata <- function(x) {
bots %>%
dplyr::select(grep(x, names(bots)))
}
businessmodels <- bots[,grepl("bm", names(bots))] #selecting only business model questions
businessmodels[businessmodels == '-99'] <- 1 #recode the bug in answers to low
businessmodels2 <- businessmodels[,colSums(is.na(businessmodels)) == 0] #remove columns that have NA's
businessmodels <- businessmodels[,!grepl("Other", names(businessmodels))]
businessmodels <- businessmodels[,1:41] #select the variables for pca
businessmodels <- businessmodels[,!grepl("HourlyRates.1", names(businessmodels))]
#First we conducetd a "parallel" analysis in order to to determine the number of components in a data matrix.
pcadat <- cor(businessmodels, use = "complete.obs") #correlation matrix
parallel <- fa.parallel(pcadat, fm = 'minres', fa = 'both', n.obs = 190)
pca <- PCA(pcadat, ncp=6, graph = FALSE)  ###tried inserting the correlation matrix instead
summary(pca)
pca$eig
####=====================================================================================
#### PCA using the prcomp function in stats package that uses the original score as input
####=====================================================================================
pca2<- prcomp(businessmodels)
####=====================================================================================
#### PCA using the prcomp function in stats package that uses the original score as input
####=====================================================================================
pca2<- prcomp(pcadat)
summary(pca2)
#The loadings matrices
loadingstwo6 <- pca2$rotation[,1:6] #for 6 components
View(loadingstwo6)
#The loadings matrices
loadingspca <- pca$var$cor # 6 is all components, because that was suggested by the analysis
View(loadingspca)
?prcomp
?PCA
?prcomp
#First we conducetd a "parallel" analysis in order to to determine the number of components in a data matrix.
pcadat <- cor(businessmodels, use = "complete.obs") #correlation matrix
parallel <- fa.parallel(pcadat, fm = 'minres', fa = 'both', n.obs = 190)
#based on the results we retain 6 principal? components
pca <- PCA(pcadat, ncp=6, graph = FALSE)  ###tried inserting the correlation matrix instead
summary(pca)
pca$eig
#The loadings matrices
loadingspca <- pca$var$cor # 6 is all components, because that was suggested by the analysis
###Component loadings on
varcontributions <- as.data.frame(loadingspca)
comp1 <- subset(varcontributions, Dim.1 >= 0.7)
comp1 <- as.vector(rownames(comp1))
comp2 <- subset(varcontributions, Dim.2 >= 0.5)
comp2 <- as.vector(rownames(comp2))
comp3 <- subset(varcontributions, Dim.3 >= 0.5)
comp3 <- as.vector(rownames(comp3))
comp4 <- subset(varcontributions, Dim.4 >= 0.45)
comp4 <- as.vector(rownames(comp4))
comp5 <- subset(varcontributions, Dim.5 >= 0.40)
comp5 <- as.vector(rownames(comp5))
comp6 <- subset(varcontributions, Dim.6 >= 0.40)
comp6 <- as.vector(rownames(comp6))
#MAKE A TABLE OF THE RELEVANT COMPONENTS
x <- as.data.frame(cbind(comp1, comp2, comp3, comp4, comp5, comp6))
x$comp2[8] <- "-"
x$comp3[5:8] <- "-"
x$comp4[3:8] <- "-"
x$comp5[2:8] <- "-"
x$comp6[3:8] <- "-"
####TABLE SHOWING WHICH VARIABLES ACCOUNT THE MOST FOR THE FEATURES OF EACH COMPONENT
print(x)
View(x)
View(x)
#The scores matrices
scorespca <- pca$ind$coord#for 6 components
View(scorespca)
View(scorespca)
summary(scorespca)
?ecdf
pcascores <- round(as.data.frame(pca$ind$coord), 2) #using coordinates as membership scores to pca
View(pca)
View(pca)
?PCA
#based on the results we retain 6 principal? components
pca <- PCA(businessmodels, ncp=6, graph = FALSE)  ###tried inserting the correlation matrix instead
summary(pca)
#The loadings matrices
loadingspca <- pca$var$cor # 6 is all components, because that was suggested by the analysis
#The scores matrices
scorespca <- pca$ind$coord#for 6 components
###Component loadings on
varcontributions <- as.data.frame(loadingspca)
View(scorespca)
View(scorespca)
summary(scorespca)
table(scorespca)
hist(scorespca)
View(scorespca)
View(scorespca)
scorespca <- as.data.frame(scorespca)
hist(scorespca)
hist(scorespca[,1])
apply(scorespca, hist, 2)
apply(scorespca, 2, hist)
#Make a data frame that only includes data for values variables
values <- bots[,grepl("values", names(bots))]
#Check the distributions for non-normalities
apply(values, 2, hist, xlab= "Likert score")
#After the examination, we took out the market value data, since they were highly skewed.
values <- values[,!grepl("Market", names(values))]
#In order to run the CFA on values set, we replicate the data in a new data frame and rename the
#variables to easier names.
mydata <- values
colnames(mydata) <- paste("X", 1:16, sep="")
performance <- bots[,grepl("Q20", names(bots))]
mydata2 <- performance
colnames(mydata2) <- paste("X", 1:23, sep="")
#After the examination, we took out the market value data, since they were highly skewed.
values <- values[,!grepl("Market", names(values))]
#Correlation matrix (lower part)
lowerCor(values)
#Finding the number of factors
fa.parallel(values) #analysis suggests 3 factors
vss(values) #3 or 4 factors
#CFA using the psych package
test.simple <- fa(r=cor(values, use="complete.obs"), nfactors=3, fm="ml", rotate="oblimin")
test.simple
fa.diagram(test.simple, cut = .4, digits = 2)
omega(values)
psych::alpha(values) #score all of the items as part of one scale.
keys.list <- list(creative=c(6,7,4,8,3,5,2), entrepreneurial=c(15,14,13,1),
financial=c(11,10,16,12,9))
myKeys <- make.keys(values,keys.list)
my.scores <- scoreItems(myKeys,values) #form several scales
print(my.scores, short = FALSE) #show the highlights of the results
setwd("~/Documents/ievasieva.github.io")
blogdown::serve_site()
blogdown::serve_site()
blogdown::serve_site()
blogdown::serve_site()
View(parallel)
View(parallel)
fa.diagram(test.simple$loadings,Phi=Phi,main="Input from a matrix")
fa.diagram(ICLUST(values,3,title="Two cluster solution"),main="Input from ICLUST")
fa.diagram(test.simple, cut = .4, digits = 2)
#using the clustering algorithm (a simpler alternative to CFA)
iclust(values)
setwd("~/Documents/ievasieva.github.io")
blogdown::serve_site()
blogdown::serve_site()
setwd("~/Documents/ievasieva.github.io/themes/hugo-creative-portfolio-theme/exampleSite")
blogdown::serve_site()
blogdown::serve_site()
setwd("~/Documents/ievasieva.github.io/themes/hugo-creative-portfolio-theme")
blogdown::serve_site()
setwd("~/Documents/ievasieva.github.io")
blogdown::serve_site()
